{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30761,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Taxi V.3\n\nThe goal of this notbook is to develp a Q-Learning algorythm for a Huggie Face RL course.","metadata":{}},{"cell_type":"code","source":"import gymnasium as gym \nimport numpy as np\nfrom random import random ","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-09-05T17:19:10.950986Z","iopub.execute_input":"2024-09-05T17:19:10.951417Z","iopub.status.idle":"2024-09-05T17:19:10.956974Z","shell.execute_reply.started":"2024-09-05T17:19:10.951375Z","shell.execute_reply":"2024-09-05T17:19:10.955603Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"env = gym.make('Taxi-v3', render_mode=\"rgb_array\")","metadata":{"execution":{"iopub.status.busy":"2024-09-05T17:06:06.369504Z","iopub.execute_input":"2024-09-05T17:06:06.370092Z","iopub.status.idle":"2024-09-05T17:06:06.392802Z","shell.execute_reply.started":"2024-09-05T17:06:06.370041Z","shell.execute_reply":"2024-09-05T17:06:06.391545Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"obervation_space = env.observation_space.n\nprint(f'The observation space is : {obervation_space}')","metadata":{"execution":{"iopub.status.busy":"2024-09-05T17:07:03.631547Z","iopub.execute_input":"2024-09-05T17:07:03.632547Z","iopub.status.idle":"2024-09-05T17:07:03.638720Z","shell.execute_reply.started":"2024-09-05T17:07:03.632498Z","shell.execute_reply":"2024-09-05T17:07:03.637455Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"The observation space is :500\n","output_type":"stream"}]},{"cell_type":"code","source":"action_space = env.action_space.n\nprint(f'The observation space is : {action_space}')","metadata":{"execution":{"iopub.status.busy":"2024-09-05T17:07:27.799546Z","iopub.execute_input":"2024-09-05T17:07:27.799998Z","iopub.status.idle":"2024-09-05T17:07:27.806835Z","shell.execute_reply.started":"2024-09-05T17:07:27.799961Z","shell.execute_reply":"2024-09-05T17:07:27.805252Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"The observation space is : 6\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Observation Space:\nThere are 500 discrete states since there are 25 taxi positions, 5 possible locations of the passenger (including the case when the passenger is in the taxi), and 4 destination locations.\n\nDestination on the map are represented with the first letter of the color.\n\nPassenger locations:\n\n   - 0: Red\n\n   - 1: Green\n\n   - 2: Yellow\n\n   - 3: Blue\n\n   - 4: In taxi\n\nDestinations:\n\n   - 0: Red\n\n   - 1: Green\n\n   - 2: Yellow\n\n   - 3: Blue\n\nAn observation is returned as an int() that encodes the corresponding state, calculated by ((taxi_row * 5 + taxi_col) * 5 + passenger_location) * 4 + destination","metadata":{}},{"cell_type":"markdown","source":"### Action Space:\nThe action shape is (1,) in the range {0, 5} indicating which direction to move the taxi or to pickup/drop off passengers.\n\n   - 0: Move south (down)\n\n   - 1: Move north (up)\n\n   - 2: Move east (right)\n\n   - 3: Move west (left)\n\n   - 4: Pickup passenger\n\n   - 5: Drop off passenger\n","metadata":{}},{"cell_type":"markdown","source":"#### Rewords:\n\n\n\n   - -1 per step unless other reward is triggered.\n\n   - +20 delivering passenger.\n\n   - -10 executing “pickup” and “drop-off” actions illegally.\n\nAn action that results a noop, like moving into a wall, will incur the time step penalty. Noops can be avoided by sampling the action_mask returned in info.","metadata":{}},{"cell_type":"markdown","source":"## Functions for Q-Learning","metadata":{}},{"cell_type":"code","source":"def init_Q_table(observation, action):\n    return np.zeros((observation, action))","metadata":{"execution":{"iopub.status.busy":"2024-09-05T17:11:39.944066Z","iopub.execute_input":"2024-09-05T17:11:39.944508Z","iopub.status.idle":"2024-09-05T17:11:39.950239Z","shell.execute_reply.started":"2024-09-05T17:11:39.944465Z","shell.execute_reply":"2024-09-05T17:11:39.948925Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"def greedy_policy(q_table, state):\n    return np.argmax(q_table[sate])","metadata":{"execution":{"iopub.status.busy":"2024-09-05T17:20:52.587690Z","iopub.execute_input":"2024-09-05T17:20:52.588100Z","iopub.status.idle":"2024-09-05T17:20:52.593767Z","shell.execute_reply.started":"2024-09-05T17:20:52.588060Z","shell.execute_reply":"2024-09-05T17:20:52.592490Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"def greedy_eps_policy(q_table, state, eps):\n    if eps >= random():\n        return env.action_space.sample()\n    else:\n        return greedy_policy(q_table, state)","metadata":{"execution":{"iopub.status.busy":"2024-09-05T17:20:50.671672Z","iopub.execute_input":"2024-09-05T17:20:50.672793Z","iopub.status.idle":"2024-09-05T17:20:50.678284Z","shell.execute_reply.started":"2024-09-05T17:20:50.672747Z","shell.execute_reply":"2024-09-05T17:20:50.677081Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"Q_table_zeros = init_Q_table(obervation_space, action_space)","metadata":{"execution":{"iopub.status.busy":"2024-09-05T17:20:56.116547Z","iopub.execute_input":"2024-09-05T17:20:56.116947Z","iopub.status.idle":"2024-09-05T17:20:56.122271Z","shell.execute_reply.started":"2024-09-05T17:20:56.116914Z","shell.execute_reply":"2024-09-05T17:20:56.120926Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"Hyper Parameters","metadata":{}},{"cell_type":"code","source":"# Training parameters\nn_training_episodes = 25000  # Total training episodes\nlearning_rate = 0.7  # Learning rate\n\n# Evaluation parameters\nn_eval_episodes = 100  # Total number of test episodes\n\n# DO NOT MODIFY EVAL_SEED\neval_seed = [\n    16,\n    54,\n    165,\n    177,\n    191,\n    191,\n    120,\n    80,\n    149,\n    178,\n    48,\n    38,\n    6,\n    125,\n    174,\n    73,\n    50,\n    172,\n    100,\n    148,\n    146,\n    6,\n    25,\n    40,\n    68,\n    148,\n    49,\n    167,\n    9,\n    97,\n    164,\n    176,\n    61,\n    7,\n    54,\n    55,\n    161,\n    131,\n    184,\n    51,\n    170,\n    12,\n    120,\n    113,\n    95,\n    126,\n    51,\n    98,\n    36,\n    135,\n    54,\n    82,\n    45,\n    95,\n    89,\n    59,\n    95,\n    124,\n    9,\n    113,\n    58,\n    85,\n    51,\n    134,\n    121,\n    169,\n    105,\n    21,\n    30,\n    11,\n    50,\n    65,\n    12,\n    43,\n    82,\n    145,\n    152,\n    97,\n    106,\n    55,\n    31,\n    85,\n    38,\n    112,\n    102,\n    168,\n    123,\n    97,\n    21,\n    83, 158, 26, 80, 63, 5, 81, 32, 11, 28,148,\n]  # Evaluation seed, this ensures that all classmates agents are trained on the same taxi starting position\n# Each seed has a specific starting state\n\n# Environment parameters\nenv_id = \"Taxi-v3\"  # Name of the environment\nmax_steps = 99  # Max steps per episode\ngamma = 0.95  # Discounting rate\n\n# Exploration parameters\nmax_epsilon = 1.0  # Exploration probability at start\nmin_epsilon = 0.05  # Minimum exploration probability\ndecay_rate = 0.005  # Exponential decay rate for exploration prob","metadata":{},"execution_count":null,"outputs":[]}]}